# -*- coding: utf-8 -*-
"""iris

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18N38ILNQgRD-82ReVmVt5atNYmViWicA
"""

import tensorflow as tf
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
from sklearn import metrics
import math

tf.logging.set_verbosity(tf.logging.ERROR)

original_dataset = pd.read_csv('https://raw.githubusercontent.com/uzairzia/iris/master/dataset/bezdekIris.data', header=None)
dataset = original_dataset.copy(deep=True)

dataset.head()

dataset.describe()

dataset_histogram = dataset.hist(figsize=(10,10))

# Compute correlation of columns with each other
dataset.corr()

# Reorder the dataset randomly
dataset = dataset.reindex(np.random.permutation(dataset.index))

# Use first 70% of the dataset for training
train_dataset = dataset.head(int(len(dataset) * 0.7))

# Use last 30% of the dataset for testing
test_dataset = dataset.tail(int(np.ceil(len(dataset) * 0.3)))

train_dataset.describe()

test_dataset.describe()

train_dataset_histogram = train_dataset.hist(figsize=(10,10))

test_dataset_histogram = test_dataset.hist(figsize=(10,10))

# Split the train dataset on the basis of class
train_dataset_setosa = train_dataset.loc[train_dataset.index[train_dataset[4] == 'Iris-setosa']]
train_dataset_versicolor = train_dataset.loc[train_dataset.index[train_dataset[4] == 'Iris-versicolor']]
train_dataset_virginica = train_dataset.loc[train_dataset.index[train_dataset[4] == 'Iris-virginica']]

# Draw scatter plot using 'Sepal Length' and 'Sepal Width'
plt.figure(figsize=(5,5))
plt.scatter(x=train_dataset_setosa.loc[:,0], y=train_dataset_setosa.loc[:,1], c='r', label='setosa')
plt.scatter(x=train_dataset_versicolor.loc[:,0], y=train_dataset_versicolor.loc[:,1], c='g', label='versicolor')
plt.scatter(x=train_dataset_virginica.loc[:,0], y=train_dataset_virginica.loc[:,1], c='b', label='viginica')
plt.xlabel('feature 0 - sepal length')
plt.ylabel('feature 1 - sepal width')
plt.legend()
plt.show()

# Extract features from dataset (first four columns) and preprocess them
def get_features(passed_dataframe):
    features = passed_dataframe.iloc[:,:4]

    # Convert features into a dictionary of numpy arrays
    # Each key:value pair represents a single feature of all examples
    features = {str(key):np.array(value) for key,value in dict(features).items()}
    
    return features

# Extract labels from dataset (last column) and preprocess them
def get_labels(passed_dataframe):
    string_labels = passed_dataframe.iloc[:,4]

    # Convert string labels to discrete numerical values
    numerical_labels = pd.Series(pd.Categorical(string_labels).codes, name=string_labels.name, dtype=np.int64)

    return numerical_labels

# Our feature columns are numeric values
def get_feature_columns(passed_dataframe):
    return set([tf.feature_column.numeric_column(str(column_name)) for column_name in passed_dataframe])

# Input function creates and returns successive batches of dataset
def input_data(features, labels, batch_size=1, epochs=None, shuffle=True):
    # Construct dataset from given features and labels
    dataset = tf.data.Dataset.from_tensor_slices((features, labels))

    # Configure batch size and epochs
    dataset = dataset.batch(batch_size, drop_remainder=True).repeat(epochs)

    if shuffle:
        dataset = dataset.shuffle(buffer_size=105)

    features, labels = dataset.make_one_shot_iterator().get_next()
    return features, labels

# Prepare features and labels for training
train_dataset_features = get_features(train_dataset)
train_dataset_labels = get_labels(train_dataset)

# Prepare features and labels for testing
test_dataset_features = get_features(test_dataset)
test_dataset_labels = get_labels(test_dataset)

# Define Optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

# Set up linear classifier
linear_classifier = tf.estimator.LinearClassifier(
    feature_columns=get_feature_columns(train_dataset_features), 
    n_classes=3, 
    optimizer=optimizer)

# Input function for training on train dataset
training_input_fn = lambda: input_data(train_dataset_features, train_dataset_labels, batch_size=15, shuffle=True)

# Train classifier on the training data
_ = linear_classifier.train(
    input_fn = training_input_fn,
    steps=1000)

# Input function for predicting on test dataset
prediction_input_fn = lambda: input_data(test_dataset_features, test_dataset_labels, epochs=1, shuffle=False)

# Predict on test dataset
predictions = linear_classifier.predict(input_fn=prediction_input_fn)
predictions = np.array([record['class_ids'][0] for record in predictions])

# Metrics for evaluating classifier
evaluation_metrics = linear_classifier.evaluate(input_fn=prediction_input_fn)
mean_squared_error = metrics.mean_squared_error(test_dataset_labels, predictions)
root_mean_squared_error = math.sqrt(mean_squared_error)

print("\nMean squared error: {}".format(mean_squared_error))
print("Root mean squared error: {}".format(root_mean_squared_error))
print("Accuracy: {}".format(evaluation_metrics['accuracy']))

# Reverse map labels used for classifier to original labels (eg. 0 -> Iris-setosa)
predictions_mapped = pd.Series(predictions).map({0:'Iris-setosa', 1:'Iris-versicolor', 2:'Iris-virginica'})

print("-Actual Labels- \t\t -Predicted Labels-\n")
for i in range(len(predictions_mapped)):
    print(test_dataset.iloc[i,4] + "\t\t" + predictions_mapped.iloc[i])